import glob
import email
from multiprocessing import Pool
from os import mkdir, path
import nltk
import random
import re
from nltk.corpus import stopwords
from nltk.stem.porter import *

def get_id(filename):
    return filename.split('.')[-1]

def get_int_id(filename):
    return int(get_id(filename))

stopset = set(stopwords.words('english'))
stemmer = PorterStemmer()

"""
list of filenames
"""
filenames = sorted(glob.glob(path.join('trec07p', 'data', 'inmail.*')), key=get_int_id)
filenameNos = map(get_id, filenames)

"""
list of strings that are either 'ham'/'spam' based on whether the email is ham/spam.
"""
checker = [''] + [line.split()[0] for line in open(path.join('trec07p', 'full', 'index'), 'r').readlines()]

"""
returns 'ham' if email is ham, else returns 'spam'
"""
def is_spam(filename):
    return checker[int(filename.split('.')[-1])]

"""
reads file with encoding latin-1
"""
def read_file(filename):
    with open(filename, 'r', encoding='latin-1') as fileObj:
        emailStr = fileObj.read()
    #print(filename)
    return emailStr

"""
given a tuple (text, filename) save it in encoding latin-1
"""
def save_file(data):
    with open(data[1], 'w') as fileObj:
        emailStr = fileObj.write(data[0])

"""
make directory if not exists
"""
def makedir(newdir):
    if not path.exists(newdir):
        mkdir(newdir)


"""
return which encoding did not produce an error out of the choices below.
if all produced an error, return 'weird'
"""
def which_encoding(filename):
    emailStr = None
    encodings = ['utf_8', 'cp1252']
    for enc in encodings:
        fileObj = open(filename, 'r', encoding=enc)
        try:
            emailStr = fileObj.read()
        except UnicodeDecodeError:
            fileObj.close()
        else:
            fileObj.close()
            return enc
    if emailStr == None:
        return 'weird'

"""
using a grouping function, return which emails in each group are spam/ham
"""
def group(grouping_function, filenames):
    groups = {}
    pool = Pool(processes=None)
    #email_groups = list(map(grouping_function, filenames))
    email_groups = pool.map(grouping_function, filenames)
    for i in range(len(email_groups)):
        grp = str(email_groups[i])
        if grp not in groups.keys():
            groups[grp] = {'spam': 0, 'ham': 0}
        groups[grp][is_spam(filenames[i])] += 1
    return groups

"""
pretty prints the output generated by group()
"""
def show_group_stats(groups):
    s = 10 
    s_g = 10
    def printl_g(string):
        print(str(string).ljust(s_g), end='')
    def printl(string):
        print(str(string).ljust(s), end='')
    def printr(string):
        print(str(string).rjust(s), end='')
    printl_g(' ')
    printl('ham')
    printl('spam')
    printl('total')
    print()
    for key in groups:
        printl_g(key)
        printr(groups[key]['ham'])
        printr(groups[key]['spam'])
        printr(sum(groups[key].values()))
        print()
    printl_g('total')
    hams = sum([groups[key]['ham'] for key in groups])
    spams = sum([groups[key]['spam'] for key in groups])
    printr(hams)
    printr(spams)
    printr(hams+spams)
    print()





patterns_to_remove = [
        re.compile('<style>.*</style>', re.S),
        re.compile('<.*?>', re.S)
]

tokenPositives = [
    re.compile('^[A-Za-z0-9\-\.]{1,30}$')
]

tokenNegatives = [
    re.compile('^[\-\.]+$')
]

# from https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch01s19.html
def multiple_replace(text, adict):
    rx = re.compile('|'.join(map(re.escape, adict)))
    def one_xlat(match):
        return adict[match.group(0)]
    return rx.sub(one_xlat, text)

#preprocess('trec07p/data/inmail.58044', encoding='utf_8')

#groups = group(which_encoding, filenames)
#show_group_stats(groups)

makedir('preprocess')

def pp_filename(filename):
    return 'preprocess/inmail.' + filename.split('.')[-1]

def preprocess(text, stop=False, stem=False):
    def email_obj_is_text(emailObj):
        return emailObj.get_content_type() == 'text/plain' or emailObj.get_content_type() == 'text/html'

    emailObj = email.message_from_string(text)
    addtl_words = []
    # from and subject attributes added
    addtl_words.append(emailObj['From'])
    if emailObj['Subject']:
        addtl_words.append(emailObj['Subject'])
    # if multipart, walk through all parts, get all text shits
    if emailObj.is_multipart():
        payloads = emailObj.walk()
        b = ' '.join((map(lambda p: p.get_payload(),
            filter(email_obj_is_text, payloads))))
        body = b
    elif email_obj_is_text(emailObj):
        body = emailObj.get_payload()
    else:
        body = emailObj.get_content_type().replace('/', '')
    for r in patterns_to_remove:
        body = re.sub(r, '', body)
    body = body.replace('\x92', "'")
    body = body.replace('=92', "'")
    body = body.replace('=20', ' ')
    body = (body + '\n' + '\n'.join(addtl_words)).lower()
    tokens = nltk.word_tokenize(body)
    for tp in tokenPositives:
        tokens = filter(tp.match, tokens)
    for tn in tokenNegatives:
        tokens = filter(lambda tok: not tn.match(tok), tokens)

    if stop:
        tokens = set(tokens) - stopset
    if stem:
        tokens = [stemmer.stem(token) for token in tokens]
    body = ' '.join(tokens)
    #body = '=================OLD:\n' + text + '==================NEW:\n' + body
    return body

